rmse_wide <- rmse_wide[match(row_order, rmse_wide$Level), ]
rmse_wide <- rmse_wide[, c("Level", column_order), with = FALSE]
# Set row names for pheatmap
rows <- c('gplearn', 'PySR', 'feyn', 'gplearn ', 'PySR ', 'feyn ', 'gplearn  ', 'PySR  ', 'feyn  ')
rmse_wide <- rmse_wide %>%
select(-Level)
rownames(rmse_wide) <- rows
colnames(rmse_wide) <- 1:10
# Plot heatmap
p <- pheatmap(as.matrix(rmse_wide), cluster_rows = FALSE, cluster_cols = FALSE,
border_color = "white", cellwidth = 20, cellheight = 20,
gaps_row = seq(3, length(row_order), by = 3), # Thicker lines every 3 rows)
main = "Variation in RMSE")  # Add title)
# ggsave(file = 'C:\\Users\\Jessie PC\\OneDrive - University of North Carolina at Chapel Hill\\Symbolic_regression_github\\NIH_Cloud_NOSI\\images\\1_Simulated_data\\All_results\\rmse_heatmap.png', p, width = 4, height = 4)
# Convert to wide format for RMSE heatmap
acc_wide <- results_all %>%
select(Input, Level, Correct_level) %>%
pivot_wider(names_from = Input, values_from = Correct_level)
# Reorder rows and columns
acc_wide <- acc_wide[match(row_order, acc_wide$Level), ]
acc_wide <- acc_wide[, c("Level", column_order), with = FALSE]
# Set row names for pheatmap
rows <- c('gplearn', 'PySR', 'feyn', 'gplearn ', 'PySR ', 'feyn ', 'gplearn  ', 'PySR  ', 'feyn  ')
acc_wide <- acc_wide %>%
select(-Level)
rownames(acc_wide) <- rows
colnames(acc_wide) <- 1:10
# Plot heatmap
p <- pheatmap(as.matrix(acc_wide),
cluster_rows = FALSE,
cluster_cols = FALSE,
border_color = "white",
cellwidth = 20,
cellheight = 20,
gaps_row = seq(3, length(row_order), by = 3),  # Thicker lines every 3 rows
main = 'Variation in Accuracy',
color = colorRampPalette(c("#CCCCFF", "#6666FF", "#0000FF", "#000066"))(4)) # Light red to red color scale
ggsave(file = 'C:\\Users\\Jessie PC\\OneDrive - University of North Carolina at Chapel Hill\\Symbolic_regression_github\\NIH_Cloud_NOSI\\images\\1_Simulated_data\\All_results\\accuracy_heatmap.png', p, width = 4, height = 4)
library(matrixStats)
library(simstudy)
library(MASS) # Ensure you have the MASS package for mvrnorm function
library(Matrix) # for nearPD function
library(reshape2)
library(tidyverse)
library(reticulate)
# Set number of samples
n <- 50
# Create an identity matrix of size 15
corr_matrix <- diag(15)
# Loop to fill the upper triangular part of the matrix with random correlations
for (i in 1:14) {
for (j in (i + 1):15) {
corr_value <- runif(1, 0.2, 0.5) * sample(c(-1, 1), 1)
corr_matrix[i, j] <- corr_value
corr_matrix[j, i] <- corr_value
}
}
# Ensure the correlation matrix is positive definite
corr_matrix <- as.matrix(nearPD(corr_matrix)$mat)
# Define mean vector for 15 variables
mu <- rep(0, 15) # Mean vector set to 0 for generating standard normal data
# Define standard deviation vector for 15 variables
sigma <- rep(1, 15) # Standard deviation set to 1 for generating standard normal data
# Create the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- diag(sigma) %*% corr_matrix %*% diag(sigma)
# Generate multivariate normal data
normal_data <- mvrnorm(n = n, mu = mu, Sigma = cov_matrix)
# Transform the normal data to the specified distributions
transformed_data <- normal_data
# Transform first 4 variables to uniform
transformed_data[, 1] <- qunif(pnorm(normal_data[, 1]), min = 1, max = 10)
transformed_data[, 2] <- qunif(pnorm(normal_data[, 2]), min = 2, max = 5)
transformed_data[, 3] <- qunif(pnorm(normal_data[, 3]), min = 4, max = 12)
transformed_data[, 4] <- qunif(pnorm(normal_data[, 4]), min = 0.5, max = 8)
# Transform next 4 variables to normal
transformed_data[, 5] <- qnorm(pnorm(normal_data[, 5]), mean = 3, sd = 1)
transformed_data[, 6] <- qnorm(pnorm(normal_data[, 6]), mean = 5, sd = 1.5)
transformed_data[, 7] <- qnorm(pnorm(normal_data[, 7]), mean = 7, sd = 2)
transformed_data[, 8] <- qnorm(pnorm(normal_data[, 8]), mean = 8, sd = 2)
# Transform next 4 variables to beta
transformed_data[, 9] <- qbeta(pnorm(normal_data[, 9]), shape1 = 2, shape2 = 5)
transformed_data[, 10] <- qbeta(pnorm(normal_data[, 10]), shape1 = 5, shape2 = 1)
transformed_data[, 11] <- qbeta(pnorm(normal_data[, 11]), shape1 = 3, shape2 = 6)
transformed_data[, 12] <- qbeta(pnorm(normal_data[, 12]), shape1 = 1, shape2 = 2)
# Transform last 3 variables to lognormal
transformed_data[, 13] <- qlnorm(pnorm(normal_data[, 13]), meanlog = 0, sdlog = 1)
transformed_data[, 14] <- qlnorm(pnorm(normal_data[, 14]), meanlog = 2, sdlog = 1)
transformed_data[, 15] <- qlnorm(pnorm(normal_data[, 15]), meanlog = 1, sdlog = 0.5)
# Convert to data frame
transformed_data <- as.data.frame(transformed_data)
# Name the columns chem1 to chem15
colnames(transformed_data) <- paste0("chem", 1:15)
# Add the response column
Response <-3 * transformed_data$chem2 * transformed_data$chem6 +
(transformed_data$chem13 / transformed_data$chem10)
transformed_data$Response <- Response
# Compute the correlation matrix of the transformed data
transformed_corr_matrix <- cor(transformed_data)
# Melt the correlation matrix for ggplot2
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)
# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
xlab(NULL)+
ylab(NULL)+
coord_fixed()
p
p
set.seed(17)
set.seed(17)
# Set number of samples
n <- 50
# Create an identity matrix of size 15
corr_matrix <- diag(15)
# Loop to fill the upper triangular part of the matrix with random correlations
for (i in 1:14) {
for (j in (i + 1):15) {
corr_value <- runif(1, 0.2, 0.5) * sample(c(-1, 1), 1)
corr_matrix[i, j] <- corr_value
corr_matrix[j, i] <- corr_value
}
}
# Ensure the correlation matrix is positive definite
corr_matrix <- as.matrix(nearPD(corr_matrix)$mat)
# Define mean vector for 15 variables
mu <- rep(0, 15) # Mean vector set to 0 for generating standard normal data
# Define standard deviation vector for 15 variables
sigma <- rep(1, 15) # Standard deviation set to 1 for generating standard normal data
# Create the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- diag(sigma) %*% corr_matrix %*% diag(sigma)
# Generate multivariate normal data
normal_data <- mvrnorm(n = n, mu = mu, Sigma = cov_matrix)
# Transform the normal data to the specified distributions
transformed_data <- normal_data
# Transform first 4 variables to uniform
transformed_data[, 1] <- qunif(pnorm(normal_data[, 1]), min = 1, max = 10)
transformed_data[, 2] <- qunif(pnorm(normal_data[, 2]), min = 2, max = 5)
transformed_data[, 3] <- qunif(pnorm(normal_data[, 3]), min = 4, max = 12)
transformed_data[, 4] <- qunif(pnorm(normal_data[, 4]), min = 0.5, max = 8)
# Transform next 4 variables to normal
transformed_data[, 5] <- qnorm(pnorm(normal_data[, 5]), mean = 3, sd = 1)
transformed_data[, 6] <- qnorm(pnorm(normal_data[, 6]), mean = 5, sd = 1.5)
transformed_data[, 7] <- qnorm(pnorm(normal_data[, 7]), mean = 7, sd = 2)
transformed_data[, 8] <- qnorm(pnorm(normal_data[, 8]), mean = 8, sd = 2)
# Transform next 4 variables to beta
transformed_data[, 9] <- qbeta(pnorm(normal_data[, 9]), shape1 = 2, shape2 = 5)
transformed_data[, 10] <- qbeta(pnorm(normal_data[, 10]), shape1 = 5, shape2 = 1)
transformed_data[, 11] <- qbeta(pnorm(normal_data[, 11]), shape1 = 3, shape2 = 6)
transformed_data[, 12] <- qbeta(pnorm(normal_data[, 12]), shape1 = 1, shape2 = 2)
# Transform last 3 variables to lognormal
transformed_data[, 13] <- qlnorm(pnorm(normal_data[, 13]), meanlog = 0, sdlog = 1)
transformed_data[, 14] <- qlnorm(pnorm(normal_data[, 14]), meanlog = 2, sdlog = 1)
transformed_data[, 15] <- qlnorm(pnorm(normal_data[, 15]), meanlog = 1, sdlog = 0.5)
# Convert to data frame
transformed_data <- as.data.frame(transformed_data)
# Name the columns chem1 to chem15
colnames(transformed_data) <- paste0("chem", 1:15)
# Add the response column
Response <-3 * transformed_data$chem2 * transformed_data$chem6 +
(transformed_data$chem13 / transformed_data$chem10)
transformed_data$Response <- Response
# Compute the correlation matrix of the transformed data
transformed_corr_matrix <- cor(transformed_data)
# Melt the correlation matrix for ggplot2
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)
# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
xlab(NULL)+
ylab(NULL)+
coord_fixed()
p
p
# Set number of samples
n <- 50
# Create an identity matrix of size 15
corr_matrix <- diag(15)
# Loop to fill the upper triangular part of the matrix with random correlations
for (i in 1:14) {
for (j in (i + 1):15) {
corr_value <- runif(1, 0.2, 0.5) * sample(c(-1, 1), 1)
corr_matrix[i, j] <- corr_value
corr_matrix[j, i] <- corr_value
}
}
# Ensure the correlation matrix is positive definite
corr_matrix <- as.matrix(nearPD(corr_matrix)$mat)
# Define mean vector for 15 variables
mu <- rep(0, 15) # Mean vector set to 0 for generating standard normal data
# Define standard deviation vector for 15 variables
sigma <- rep(1, 15) # Standard deviation set to 1 for generating standard normal data
# Create the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- diag(sigma) %*% corr_matrix %*% diag(sigma)
# Generate multivariate normal data
normal_data <- mvrnorm(n = n, mu = mu, Sigma = cov_matrix)
# Transform the normal data to the specified distributions
transformed_data <- normal_data
# Transform first 4 variables to uniform
transformed_data[, 1] <- qunif(pnorm(normal_data[, 1]), min = 1, max = 10)
transformed_data[, 2] <- qunif(pnorm(normal_data[, 2]), min = 2, max = 5)
transformed_data[, 3] <- qunif(pnorm(normal_data[, 3]), min = 4, max = 12)
transformed_data[, 4] <- qunif(pnorm(normal_data[, 4]), min = 0.5, max = 8)
# Transform next 4 variables to normal
transformed_data[, 5] <- qnorm(pnorm(normal_data[, 5]), mean = 3, sd = 1)
transformed_data[, 6] <- qnorm(pnorm(normal_data[, 6]), mean = 5, sd = 1.5)
transformed_data[, 7] <- qnorm(pnorm(normal_data[, 7]), mean = 7, sd = 2)
transformed_data[, 8] <- qnorm(pnorm(normal_data[, 8]), mean = 8, sd = 2)
# Transform next 4 variables to beta
transformed_data[, 9] <- qbeta(pnorm(normal_data[, 9]), shape1 = 2, shape2 = 5)
transformed_data[, 10] <- qbeta(pnorm(normal_data[, 10]), shape1 = 5, shape2 = 1)
transformed_data[, 11] <- qbeta(pnorm(normal_data[, 11]), shape1 = 3, shape2 = 6)
transformed_data[, 12] <- qbeta(pnorm(normal_data[, 12]), shape1 = 1, shape2 = 2)
# Transform last 3 variables to lognormal
transformed_data[, 13] <- qlnorm(pnorm(normal_data[, 13]), meanlog = 0, sdlog = 1)
transformed_data[, 14] <- qlnorm(pnorm(normal_data[, 14]), meanlog = 2, sdlog = 1)
transformed_data[, 15] <- qlnorm(pnorm(normal_data[, 15]), meanlog = 1, sdlog = 0.5)
# Convert to data frame
transformed_data <- as.data.frame(transformed_data)
# Name the columns chem1 to chem15
colnames(transformed_data) <- paste0("chem", 1:15)
# Add the response column
Response <-3 * transformed_data$chem2 * transformed_data$chem6 +
(transformed_data$chem13 / transformed_data$chem10)
transformed_data$Response <- Response
# Compute the correlation matrix of the transformed data
transformed_corr_matrix <- cor(transformed_data)
# Melt the correlation matrix for ggplot2
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)
# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
xlab(NULL)+
ylab(NULL)+
coord_fixed()
p
# ggsave(filename = '..\\images\\1_Simulated_data\\Input_data\\Input_correlations.png', p)
# Load the transformed data from the CSV file
# transformed_data = pd.read_csv("transformed_data.csv")
# Create dataframe only containing variables included in the equation
sim_dat_rel = transformed_data %>%
select(chem2, chem6, chem10, chem13, Response)
# Save the simulated dataframes
# transformed_data.to_pickle("Data_inputs/1_Simulated_data/sim_dat_all.pkl")
# sim_dat_rel.to_pickle("Data_inputs/1_Simulated_data/sim_dat_rel.pkl")
# Initialize dictionary to hold noisy dataframes
sim_noise_dict = list()
# Add noise to the response variable in the complete dataset
std = 0.25
while(std<2.25){
#Generate noise from a Gaussian distribution with mean 0 and standard deviation i+1
noise <- rnorm(n, mean = 0, sd = std)
response_noisy = Response + noise
# Update dataframe to have noisy response
sim_noise_temp = transformed_data
sim_noise_temp['Response'] = response_noisy
# Append to dictionary
sim_noise_dict[[paste0('Noise=', std)]] <- sim_noise_temp
# Increase standard deviation
std = std + 0.25
}
# Combine all dataframes into one list to iterate through
sim_dict = sim_noise_dict
sim_dict[['No_noise_all_var']] = transformed_data
sim_dict[['No_noise_rel_var']] = sim_dat_rel
reticulate::repl_python()
import pandas as pd
import pickle
# Save the combined dictionary of simulated dataframes
sim_dict = r.sim_dict
with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'wb') as f:
pickle.dump(sim_dict, f)
import pandas as pd
import pickle
# Save the combined dictionary of simulated dataframes
sim_dict = r.sim_dict
with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'wb') as f:
pickle.dump(sim_dict, f)
with open('Data_inputs/1_Simulated_data/sim_dict.pkl', 'rb') as f:
sim_dict = pickle.load(f)
with open('C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'rb') as f:
sim_dict = pickle.load(f)
with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'rb') as f:
sim_dict = pickle.load(f)
View(sim_dict)
quit
sim_dict <- py.sim_dict
sim_dict <- py$sim_dict
View(sim_dict)
View(melted_transformed_corr_matrix)
View(sim_dict[["No_noise_all_var"]])
rel <- sim_dict$No_noise_all_var
View(rel)
transformed_corr_matrix <- cor(rel)
View(transformed_corr_matrix)
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)
# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
xlab(NULL)+
ylab(NULL)+
coord_fixed()
p
library(matrixStats)
library(simstudy)
library(MASS) # Ensure you have the MASS package for mvrnorm function
library(Matrix) # for nearPD function
library(reshape2)
library(tidyverse)
library(reticulate)
library(matrixStats)
library(simstudy)
library(MASS) # Ensure you have the MASS package for mvrnorm function
library(Matrix) # for nearPD function
library(reshape2)
library(tidyverse)
library(reticulate)
# Set seed
set.seed(17)
# Set number of samples
n <- 50
# Create an identity matrix of size 15
corr_matrix <- diag(15)
# Loop to fill the upper triangular part of the matrix with random correlations
for (i in 1:14) {
for (j in (i + 1):15) {
corr_value <- runif(1, 0.2, 0.5) * sample(c(-1, 1), 1)
corr_matrix[i, j] <- corr_value
corr_matrix[j, i] <- corr_value
}
}
# Ensure the correlation matrix is positive definite
corr_matrix <- as.matrix(nearPD(corr_matrix)$mat)
# Define mean vector for 15 variables
mu <- rep(0, 15) # Mean vector set to 0 for generating standard normal data
# Define standard deviation vector for 15 variables
sigma <- rep(1, 15) # Standard deviation set to 1 for generating standard normal data
# Create the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- diag(sigma) %*% corr_matrix %*% diag(sigma)
# Generate multivariate normal data
normal_data <- mvrnorm(n = n, mu = mu, Sigma = cov_matrix)
# Transform the normal data to the specified distributions
transformed_data <- normal_data
# Transform first 4 variables to uniform
transformed_data[, 1] <- qunif(pnorm(normal_data[, 1]), min = 1, max = 10)
transformed_data[, 2] <- qunif(pnorm(normal_data[, 2]), min = 2, max = 5)
transformed_data[, 3] <- qunif(pnorm(normal_data[, 3]), min = 4, max = 12)
transformed_data[, 4] <- qunif(pnorm(normal_data[, 4]), min = 0.5, max = 8)
# Transform next 4 variables to normal
transformed_data[, 5] <- qnorm(pnorm(normal_data[, 5]), mean = 3, sd = 1)
transformed_data[, 6] <- qnorm(pnorm(normal_data[, 6]), mean = 5, sd = 1.5)
transformed_data[, 7] <- qnorm(pnorm(normal_data[, 7]), mean = 7, sd = 2)
transformed_data[, 8] <- qnorm(pnorm(normal_data[, 8]), mean = 8, sd = 2)
# Transform next 4 variables to beta
transformed_data[, 9] <- qbeta(pnorm(normal_data[, 9]), shape1 = 2, shape2 = 5)
transformed_data[, 10] <- qbeta(pnorm(normal_data[, 10]), shape1 = 5, shape2 = 1)
transformed_data[, 11] <- qbeta(pnorm(normal_data[, 11]), shape1 = 3, shape2 = 6)
transformed_data[, 12] <- qbeta(pnorm(normal_data[, 12]), shape1 = 1, shape2 = 2)
# Transform last 3 variables to lognormal
transformed_data[, 13] <- qlnorm(pnorm(normal_data[, 13]), meanlog = 0, sdlog = 1)
transformed_data[, 14] <- qlnorm(pnorm(normal_data[, 14]), meanlog = 2, sdlog = 1)
transformed_data[, 15] <- qlnorm(pnorm(normal_data[, 15]), meanlog = 1, sdlog = 0.5)
# Convert to data frame
transformed_data <- as.data.frame(transformed_data)
# Name the columns chem1 to chem15
colnames(transformed_data) <- paste0("chem", 1:15)
# Add the response column
Response <-3 * transformed_data$chem2 * transformed_data$chem6 +
(transformed_data$chem13 / transformed_data$chem10)
transformed_data$Response <- Response
# Compute the correlation matrix of the transformed data
transformed_corr_matrix <- cor(transformed_data)
# Melt the correlation matrix for ggplot2
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)
# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
geom_tile(color = "white") +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
xlab(NULL)+
ylab(NULL)+
coord_fixed()
p
ggsave(filename = '..\\NIH_Cloud_NOSI\\5_Plots\\1_Simulated_data\\Input_data\\Input_correlations.png', p)
ggsave(filename = "../../5_Plots/1_Simulated_data/Input_data/Input_correlations.png", plot = p)
# Create dataframe only containing variables included in the equation
sim_dat_rel = transformed_data %>%
select(chem2, chem6, chem10, chem13, Response)
# Save the simulated dataframes
# transformed_data.to_pickle("Data_inputs/1_Simulated_data/sim_dat_all.pkl")
# sim_dat_rel.to_pickle("Data_inputs/1_Simulated_data/sim_dat_rel.pkl")
# Initialize dictionary to hold noisy dataframes
sim_noise_dict = list()
# Add noise to the response variable in the complete dataset
std = 0.25
while(std<2.25){
#Generate noise from a Gaussian distribution with mean 0 and standard deviation i+1
noise <- rnorm(n, mean = 0, sd = std)
response_noisy = Response + noise
# Update dataframe to have noisy response
sim_noise_temp = transformed_data
sim_noise_temp['Response'] = response_noisy
# Append to dictionary
sim_noise_dict[[paste0('Noise=', std)]] <- sim_noise_temp
# Increase standard deviation
std = std + 0.25
}
# Combine all dataframes into one list to iterate through
sim_dict = sim_noise_dict
sim_dict[['No_noise_all_var']] = transformed_data
sim_dict[['No_noise_rel_var']] = sim_dat_rel
reticulate::repl_python()
import pandas as pd
import pickle
# Save the combined dictionary of simulated dataframes
sim_dict = r.sim_dict
with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\3_Data_intermediates\1_Simulated_data\sim_dict.pkl', 'wb') as f:
pickle.dump(sim_dict, f)
# with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'rb') as f:
#     sim_dict = pickle.load(f)
library(data.table)
library(pheatmap)
library(tidyverse)
library(gridExtra)
library(ggpubr)
library(data.table)
library(pheatmap)
library(tidyverse)
library(gridExtra)
library(ggpubr)
setwd("C://Users//Jessie PC//OneDrive - University of North Carolina at Chapel Hill//Symbolic_regression_github//NIH_Cloud_NOSI")
# Read in results
results_pysr <- read.csv('..\\NIH_Cloud_NOSI\\4_Model_results\\1_Simulated_data\\pysr\\results_pysr.csv')
results_gplearn <- read.csv('..\\NIH_Cloud_NOSI\\4_Model_results\\1_Simulated_data\\gplearn\\results_gplearn.csv')
results_feyn <- read.csv('..\\NIH_Cloud_NOSI\\4_Model_results\\1_Simulated_data\\feyn\\results_feyn.csv')
# Combine results into one data frame
results_list <- list(results_gplearn, results_pysr, results_feyn)
names(results_list) <- c('gplearn', 'pysr', 'feyn')
View(results_feyn)
# Combine results into one data frame
results_list <- list(results_gplearn, results_pysr, results_feyn)
names(results_list) <- c('gplearn', 'pysr', 'feyn')
# Add operator complexity
for(i in 1:length(results_list)){
temp <- results_list[[i]]
# Make operator column consistent
if(i==2){
temp <- temp %>%
mutate(Operators = paste0(Binary.operators, Unary.Operators)) %>%
select(-Binary.operators, -Unary.Operators)
}
# Add complexity level
temp$Operator_complexity <- 'Low'
temp$Operator_complexity[11:20] <- 'Medium'
temp$Operator_complexity[21:30] <- 'High'
# Add package name
temp$Package <- names(results_list)[i]
# Append to list
results_list[[i]] <- temp
}
# Combine into one data frame
results_all <- do.call(bind_rows, results_list)
results_all$Comp_Level <- paste0(results_all$Package, results_all$Operator_complexity)
```
# Convert to wide format for RMSE heatmap
rmse_wide <- results_all %>%
select(Input, Comp_Level, RMSE) %>%
pivot_wider(names_from = Input, values_from = RMSE)
# Define the desired order for rows and columns
row_order <- c('gplearnHigh', 'pysrHigh', 'feynHigh', 'gplearnMedium', 'pysrMedium', 'feynMedium', 'gplearnLow','pysrLow','feynLow')
column_order <- c('No_noise_rel_var', 'No_noise_all_var', 'Noise=0.25', 'Noise=0.5', 'Noise=0.75', 'Noise=1', 'Noise=1.25', 'Noise=1.5', 'Noise=1.75', 'Noise=2')
# Reorder rows and columns
rmse_wide <- rmse_wide[match(row_order, rmse_wide$Comp_Level), ]
rmse_wide <- rmse_wide[, c("Comp_Level", column_order), with = FALSE]
# Set row names for pheatmap
rows <- c('gplearn', 'PySR', 'feyn', 'gplearn ', 'PySR ', 'feyn ', 'gplearn  ', 'PySR  ', 'feyn  ')
rmse_wide <- rmse_wide %>%
select(-Comp_Level)
rownames(rmse_wide) <- rows
colnames(rmse_wide) <- 1:10
# Plot heatmap
p <- pheatmap(as.matrix(rmse_wide), cluster_rows = FALSE, cluster_cols = FALSE,
border_color = "white", cellwidth = 20, cellheight = 20,
gaps_row = seq(3, length(row_order), by = 3), # Thicker lines every 3 rows)
main = "Variation in RMSE")  # Add title)

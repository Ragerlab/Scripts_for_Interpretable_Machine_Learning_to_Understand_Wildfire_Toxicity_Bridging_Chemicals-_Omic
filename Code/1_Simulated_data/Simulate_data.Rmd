---
title: "Untitled"
output: html_document
date: "2024-08-01"
---

# Libraries
```{r setup, include=FALSE}
library(matrixStats)
library(simstudy)
library(MASS) # Ensure you have the MASS package for mvrnorm function
library(Matrix) # for nearPD function
library(reshape2)
library(tidyverse)
library(reticulate)
```

```{r}
# Set number of samples
n <- 50

# Create an identity matrix of size 15
corr_matrix <- diag(15)

# Loop to fill the upper triangular part of the matrix with random correlations
for (i in 1:14) {
  for (j in (i + 1):15) {
    corr_value <- runif(1, 0.2, 0.5) * sample(c(-1, 1), 1)
    corr_matrix[i, j] <- corr_value
    corr_matrix[j, i] <- corr_value
  }
}

# Ensure the correlation matrix is positive definite
corr_matrix <- as.matrix(nearPD(corr_matrix)$mat)

# Define mean vector for 15 variables
mu <- rep(0, 15) # Mean vector set to 0 for generating standard normal data

# Define standard deviation vector for 15 variables
sigma <- rep(1, 15) # Standard deviation set to 1 for generating standard normal data

# Create the covariance matrix from the correlation matrix and standard deviations
cov_matrix <- diag(sigma) %*% corr_matrix %*% diag(sigma)

# Generate multivariate normal data
normal_data <- mvrnorm(n = n, mu = mu, Sigma = cov_matrix)

# Transform the normal data to the specified distributions
transformed_data <- normal_data

# Transform first 4 variables to uniform
transformed_data[, 1] <- qunif(pnorm(normal_data[, 1]), min = 1, max = 10)
transformed_data[, 2] <- qunif(pnorm(normal_data[, 2]), min = 2, max = 5)
transformed_data[, 3] <- qunif(pnorm(normal_data[, 3]), min = 4, max = 12)
transformed_data[, 4] <- qunif(pnorm(normal_data[, 4]), min = 0.5, max = 8)

# Transform next 4 variables to normal
transformed_data[, 5] <- qnorm(pnorm(normal_data[, 5]), mean = 3, sd = 1)
transformed_data[, 6] <- qnorm(pnorm(normal_data[, 6]), mean = 5, sd = 1.5)
transformed_data[, 7] <- qnorm(pnorm(normal_data[, 7]), mean = 7, sd = 2)
transformed_data[, 8] <- qnorm(pnorm(normal_data[, 8]), mean = 8, sd = 2)

# Transform next 4 variables to beta
transformed_data[, 9] <- qbeta(pnorm(normal_data[, 9]), shape1 = 2, shape2 = 5)
transformed_data[, 10] <- qbeta(pnorm(normal_data[, 10]), shape1 = 5, shape2 = 1)
transformed_data[, 11] <- qbeta(pnorm(normal_data[, 11]), shape1 = 3, shape2 = 6)
transformed_data[, 12] <- qbeta(pnorm(normal_data[, 12]), shape1 = 1, shape2 = 2)

# Transform last 3 variables to lognormal
transformed_data[, 13] <- qlnorm(pnorm(normal_data[, 13]), meanlog = 0, sdlog = 1)
transformed_data[, 14] <- qlnorm(pnorm(normal_data[, 14]), meanlog = 2, sdlog = 1)
transformed_data[, 15] <- qlnorm(pnorm(normal_data[, 15]), meanlog = 1, sdlog = 0.5)

# Convert to data frame
transformed_data <- as.data.frame(transformed_data)

# Name the columns chem1 to chem15
colnames(transformed_data) <- paste0("chem", 1:15)

# Add the response column
Response <-3 * transformed_data$chem2 * transformed_data$chem6 + 
                            (transformed_data$chem13 / transformed_data$chem10)
transformed_data$Response <- Response

# Compute the correlation matrix of the transformed data
transformed_corr_matrix <- cor(transformed_data)

# Melt the correlation matrix for ggplot2
melted_transformed_corr_matrix <- melt(transformed_corr_matrix)

# Plot the heatmap using ggplot2
p <- ggplot(data = melted_transformed_corr_matrix, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Correlation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 10, hjust = 1),
        axis.text.y = element_text(vjust = 1, size = 10, hjust = 1)) +
  xlab(NULL)+
  ylab(NULL)+
  coord_fixed()
p
ggsave(filename = '..\\images\\1_Simulated_data\\Input_data\\Input_correlations.png', p)

# Load the transformed data from the CSV file
# transformed_data = pd.read_csv("transformed_data.csv")


# Create dataframe only containing variables included in the equation
sim_dat_rel = transformed_data %>%
  select(chem2, chem6, chem10, chem13, Response)

# Save the simulated dataframes
# transformed_data.to_pickle("Data_inputs/1_Simulated_data/sim_dat_all.pkl")
# sim_dat_rel.to_pickle("Data_inputs/1_Simulated_data/sim_dat_rel.pkl")

# Initialize dictionary to hold noisy dataframes
sim_noise_dict = list()

# Add noise to the response variable in the complete dataset
std = 0.25
while(std<2.25){
  
   #Generate noise from a Gaussian distribution with mean 0 and standard deviation i+1
    noise <- rnorm(n, mean = 0, sd = std)
    response_noisy = Response + noise

    # Update dataframe to have noisy response
    sim_noise_temp = transformed_data
    sim_noise_temp['Response'] = response_noisy
    
    # Append to dictionary
    sim_noise_dict[[paste0('Noise=', std)]] <- sim_noise_temp

    # Increase standard deviation 
    std = std + 0.25
}
    
# Combine all dataframes into one list to iterate through
sim_dict = sim_noise_dict
sim_dict[['No_noise_all_var']] = transformed_data
sim_dict[['No_noise_rel_var']] = sim_dat_rel
```

# Save to python file for downstream analysis
```{python}
import pandas as pd
import pickle

# Save the combined dictionary of simulated dataframes
sim_dict = r.sim_dict
with open(r'C:\Users\Jessie PC\OneDrive - University of North Carolina at Chapel Hill\Symbolic_regression_github\NIH_Cloud_NOSI\Data_inputs\1_Simulated_data\sim_dict.pkl', 'wb') as f:
    pickle.dump(sim_dict, f)

```


